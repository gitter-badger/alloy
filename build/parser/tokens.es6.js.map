{"version":3,"sources":["../../source/parser/tokens.es6.js"],"names":[],"mappings":";;AAiDA,IAAI,MAAc,GAAG,CAEpB;AACC,cAAa,EAAE,CAAC,GAAG,CAAC;CACpB;;;;AAID;AACC,mBAAkB,EAAE;;AAEnB,OAAK,EAAE,CACN,QAAQ,EACR,MAAM,EACN,IAAI,EACJ,QAAQ,EACR,SAAS,CACT;;AAED,WAAS,EAAE,CACV,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,CACH;;EAED;CACD;;;;AAID;AACC,kBAAiB,EAAE;;AAElB,SAAO,EAAE,UAAC,IAAI,EAAE,IAAI,EAAK;AACxB,UAAO,EAAE,CAAA;GACT;;AAED,OAAK,EAAE,UAAC,IAAI,EAAE,IAAI,EAAK;AACtB,UAAO,EAAE,CAAA;GACT;;EAED;CACD,CAED,CAAC;;AAEF,MAAM,CAAC,OAAO,GAAG,MAAM,CAAC,MAAM,CAAC,MAAM,CAAC,CAAA","file":"source/parser/tokens.es6.js","sourcesContent":["/* @flow\n\ntokens.es6.js\n\nCreated by Chris Prucha\nÂ© 2015 Notion Labs, Inc\n\nProvide a list of tokens for the lexer to analyze.\n\nES6 RC3 Module System Examples:\n\n\tExample imports\n\t\timport v from \"mod\";\n\t\timport * as ns from \"mod\";\n\t\timport {x} from \"mod\";\n\t\timport {x as v} from \"mod\";\n\t\timport \"mod\";\n\n\tExample exports\n\t\texport var v;\n\t\texport default function f(){};\n\t\texport default function(){};\n\t\texport default 42;\n\t\texport {x};\n\t\texport {v as x};\n\t\texport {x} from \"mod\";\n\t\texport {v as x} from \"mod\";\n\t\texport * from \"mod\";\n\n*/\n\ntype IToken = [\n\t{\n\t\tignore_tokens: Array<string>;\n\t},\n\t{\n\t\tdeclarative_tokens: {\n\t\t\tnames    : Array<string>;\n\t\t\toperators: Array<string>;\n\t\t}\n\t},\n\t{\n\t\tprocedural_tokens: {\n\t\t\tstrings: (char: string, next: () => string) => string;\n\t\t\tnames  : (char: string, next: () => string) => string;\n\t\t}\n\t}\n];\n\nvar Tokens: IToken = [\n\n\t{\n\t\tignore_tokens: [\" \"]\n\t},\n\n\t// Todo(Chris):\n\t// Match EOL for automatic semicolon insertion\n\t{\n\t\tdeclarative_tokens: {\n\n\t\t\tnames: [\n\t\t\t\t\"import\",\n\t\t\t\t\"from\",\n\t\t\t\t\"as\",\n\t\t\t\t\"export\",\n\t\t\t\t\"default\"\n\t\t\t],\n\n\t\t\toperators: [\n\t\t\t\t\"*\",\n\t\t\t\t\"{\",\n\t\t\t\t\"}\",\n\t\t\t\t\",\",\n\t\t\t\t\";\"\n\t\t\t]\n\n\t\t}\n\t},\n\n\t// Todo(Chris):\n\t// Implement procedural tokens\n\t{\n\t\tprocedural_tokens: {\n\n\t\t\tstrings: (char, next) => {\n\t\t\t\treturn \"\"\n\t\t\t},\n\n\t\t\tnames: (char, next) => {\n\t\t\t\treturn \"\"\n\t\t\t}\n\n\t\t}\n\t}\n\n];\n\nmodule.exports = Object.freeze(Tokens)"]}